{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/ericluo04/Documents/GitHub/Bots-Project/Code/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"2. HK Training/labeling/Data/labeled_data_balanced.csv\")\n",
    "df['tweet'] = df['tweet'].apply(nltk.word_tokenize)\n",
    "stemmer = PorterStemmer()\n",
    "df['tweet'] = df['tweet'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['label'], train_size=.8)\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "cv = CountVectorizer(strip_accents='unicode', stop_words=frozenset(stops + ['hong', 'kong', 'hongkong', 'thi', 'rt', 'thank', 'hk', 'hongkongprotest', 'hkprotest', 'u2026', 'uff0c', 'http', 'co']), min_df=2)\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9204301075268817\n",
      "Precision score:  0.9354838709677419\n",
      "Recall score:  0.9049645390070922\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_cv, y_train)\n",
    "predictions = naive_bayes.predict(X_test_cv)\n",
    "\n",
    "print('Accuracy score: ', accuracy_score(y_test, predictions))\n",
    "print('Precision score: ', precision_score(y_test, predictions))\n",
    "print('Recall score: ', recall_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the 10 most predictive features for a pro-Hong Kong tweet: ['freedom' 'support' 'standwithhk' 'nba' 'china' 'fightforfreedom' 'polic'\n",
      " 'freehongkong' 'chinazi' 'standwithhongkong']\n",
      "These are the 10 most predictive features for a pro-China tweet: ['u7684' 'u66b4' 'hkriot' 'hongkongriot' 'polic' 'rioter' 'u9999' 'u6e2f'\n",
      " 'stophkriot' 'hkrioter']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted = naive_bayes.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted = naive_bayes.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "print('These are the 10 most predictive features for a pro-Hong Kong tweet:', np.take(cv.get_feature_names(), neg_class_prob_sorted[-10:]))\n",
    "print('These are the 10 most predictive features for a pro-China tweet:', np.take(cv.get_feature_names(), pos_class_prob_sorted[-10:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

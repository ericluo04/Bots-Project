{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Setting Up Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ericluo04\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ericluo04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "\n",
    "# to read in csv's and work in a python dataframe format (standard)\n",
    "import pandas as pd\n",
    "# general math processing package (standard)\n",
    "import numpy as np\n",
    "# standard nlp tool used for data cleaning\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "# package for string manipulations (standard)\n",
    "import string\n",
    "# package for regular expressions (standard)\n",
    "import re\n",
    "\n",
    "# one of the state-of-the-art NLP packages (includes sentiment analysis)\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "# textblob is a versatile package (can do sentiment analysis, but also part of speech tagging and subjectivity analysis)\n",
    "from textblob import TextBlob\n",
    "# using nltk's vader sentiment analysis tool\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# google translator\n",
    "from googletrans import Translator\n",
    "# snow NLP for simplified chinese\n",
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "path = \"C:/Users/ericluo04/Documents/GitHub/Bots-Project/Code/\"\n",
    "\n",
    "# read in .csv file as a dataframe\n",
    "df1 = pd.read_csv(path + \"2. HK Training/polarities/master_new_1.csv\")\n",
    "df2 = pd.read_csv(path + \"2. HK Training/polarities/master_new_2.csv\")\n",
    "\n",
    "df=df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_text'] = [(x.encode('utf-8')).decode('utf-8') for x in df['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>n_friends</th>\n",
       "      <th>n_tweets_user</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1174478413253545984</td>\n",
       "      <td>1174478413253545984</td>\n",
       "      <td>#inners Chris^ talk re: Pelosi^s skills\\u2014s...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:20:49</td>\n",
       "      <td>2856307798</td>\n",
       "      <td>\\U0001f308#ERAnow!!!gerrymand cost 16\\U0</td>\n",
       "      <td>WebOften</td>\n",
       "      <td>Just another #feminist\\u2640\\ufe0f #green \\U00...</td>\n",
       "      <td></td>\n",
       "      <td>2462.0</td>\n",
       "      <td>5002.0</td>\n",
       "      <td>82945.0</td>\n",
       "      <td>0.444381</td>\n",
       "      <td>#inners Chris^ talk re: Pelosi^s skills\\u2014s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1174478279719260160</td>\n",
       "      <td>1174478279719260160</td>\n",
       "      <td>@atri_septiani rasa rasanya udah eneg ngeliat ...</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-09-19 00:20:17</td>\n",
       "      <td>134116652</td>\n",
       "      <td>hari pramuka</td>\n",
       "      <td>hadipramudika</td>\n",
       "      <td>kalo ngepil harus digerus dulu</td>\n",
       "      <td></td>\n",
       "      <td>146.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5666.0</td>\n",
       "      <td>0.709279</td>\n",
       "      <td>@atri_septiani rasa rasanya udah eneg ngeliat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1174478237105348608</td>\n",
       "      <td>1174478237105348608</td>\n",
       "      <td>Today^s meeting between #HongKongProtesters an...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:20:07</td>\n",
       "      <td>2573661730</td>\n",
       "      <td>Lucas Notch</td>\n",
       "      <td>lucas_and_mew</td>\n",
       "      <td>Gamer. Otaku.</td>\n",
       "      <td></td>\n",
       "      <td>66.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>7558.0</td>\n",
       "      <td>0.196698</td>\n",
       "      <td>Today^s meeting between #HongKongProtesters an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1174478213466206208</td>\n",
       "      <td>1174478213466206208</td>\n",
       "      <td>The harder the conflict, the more glorious the...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:20:01</td>\n",
       "      <td>4782149654</td>\n",
       "      <td>Libertalia!</td>\n",
       "      <td>libertalia_band</td>\n",
       "      <td>Libertalia writes, records and performs modern...</td>\n",
       "      <td>La R\\xe9publique de Libertalia</td>\n",
       "      <td>7990.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14227.0</td>\n",
       "      <td>0.968098</td>\n",
       "      <td>The harder the conflict, the more glorious the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1174477984620630016</td>\n",
       "      <td>1174477984620630016</td>\n",
       "      <td>RT @OscarLe27560696: China own news paper (HK)...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:19:06</td>\n",
       "      <td>1126143126605746176</td>\n",
       "      <td>sanshuicc\\U0001f1ed\\U0001f1f0\\U0001f64f</td>\n",
       "      <td>sanshuicc1</td>\n",
       "      <td>\\u4f20\\u64ad\\u7206\\u6599\\u9769\\u547d</td>\n",
       "      <td></td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.687024</td>\n",
       "      <td>RT @OscarLe27560696: China own news paper (HK)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208520</td>\n",
       "      <td>1188131237719724034</td>\n",
       "      <td>1188131237719724034</td>\n",
       "      <td>\\u9032\\u5316\\u7248\\u7684\\u300c\\u8fd4\\u6821\\u30...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 16:32:16</td>\n",
       "      <td>804543102132158464</td>\n",
       "      <td>\\u527f\\u532a\\u5b78\\u9662</td>\n",
       "      <td>laichinan</td>\n",
       "      <td>\\u527f\\u532a\\u5c1a\\u672a\\u6210\\u529f\\uff0c\\u54...</td>\n",
       "      <td>Taichung City, Taiwan</td>\n",
       "      <td>5770.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>5808.0</td>\n",
       "      <td>0.746148</td>\n",
       "      <td>\\u9032\\u5316\\u7248\\u7684\\u300c\\u8fd4\\u6821\\u30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208521</td>\n",
       "      <td>1188121135239483395</td>\n",
       "      <td>1188121135239483395</td>\n",
       "      <td>\\u9999\\u6e2f\\u3001\\u30c7\\u30e2\\u3084\\u96c6\\u4f...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 15:52:07</td>\n",
       "      <td>157583088</td>\n",
       "      <td>Shota T</td>\n",
       "      <td>sasakurex</td>\n",
       "      <td>\\u6620\\u753b\\u30fb\\u97f3\\u697d\\u30fb\\u30a2\\u30...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>122.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>0.746148</td>\n",
       "      <td>\\u9999\\u6e2f\\u3001\\u30c7\\u30e2\\u3084\\u96c6\\u4f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208522</td>\n",
       "      <td>1188115606098206720</td>\n",
       "      <td>1188115606098206720</td>\n",
       "      <td>@3Yannn @Dreamy31875054 \\u9999\\u6e2f\\u306e\\u65...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 15:30:09</td>\n",
       "      <td>1168070637329346560</td>\n",
       "      <td>FreedOmHK @ \\U0001f1ef\\U0001f1f5\\U0001f1</td>\n",
       "      <td>jhm_freedom</td>\n",
       "      <td>I^m JP and Hongkong Mixd , I^m supportive HK!!...</td>\n",
       "      <td></td>\n",
       "      <td>265.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.746148</td>\n",
       "      <td>@3Yannn @Dreamy31875054 \\u9999\\u6e2f\\u306e\\u65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208523</td>\n",
       "      <td>1187980605394276352</td>\n",
       "      <td>1187980605394276352</td>\n",
       "      <td>\\u5149\\u5fa9\\u9999\\u6e2f https://t.co/c8hS9MwuWf</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 06:33:42</td>\n",
       "      <td>908206346490626048</td>\n",
       "      <td>Lazijaja</td>\n",
       "      <td>lazijajaja</td>\n",
       "      <td>Sleepy</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100058</td>\n",
       "      <td>\\u5149\\u5fa9\\u9999\\u6e2f https://t.co/c8hS9MwuWf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208524</td>\n",
       "      <td>1187944830049013760</td>\n",
       "      <td>1187944830049013760</td>\n",
       "      <td>\\u300a\\u5149\\u5fa9\\u9999\\u6e2f\\u300bVR\\u30b1\\u...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 04:11:33</td>\n",
       "      <td>4401027434</td>\n",
       "      <td>Carl Chan @\\u9999\\u6e2f\\u4fdd\\u885b\\u623</td>\n",
       "      <td>CarlChan19</td>\n",
       "      <td>\\u6240\\u5728\\u5730\\uff1a\\u9999\\u6e2f\\u3002\\n\\u...</td>\n",
       "      <td></td>\n",
       "      <td>687.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>0.746148</td>\n",
       "      <td>\\u300a\\u5149\\u5fa9\\u9999\\u6e2f\\u300bVR\\u30b1\\u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417051 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Unnamed: 0                   id  \\\n",
       "0       1174478413253545984  1174478413253545984   \n",
       "1       1174478279719260160  1174478279719260160   \n",
       "2       1174478237105348608  1174478237105348608   \n",
       "3       1174478213466206208  1174478213466206208   \n",
       "4       1174477984620630016  1174477984620630016   \n",
       "...                     ...                  ...   \n",
       "208520  1188131237719724034  1188131237719724034   \n",
       "208521  1188121135239483395  1188121135239483395   \n",
       "208522  1188115606098206720  1188115606098206720   \n",
       "208523  1187980605394276352  1187980605394276352   \n",
       "208524  1187944830049013760  1187944830049013760   \n",
       "\n",
       "                                                    tweet language  \\\n",
       "0       #inners Chris^ talk re: Pelosi^s skills\\u2014s...       en   \n",
       "1       @atri_septiani rasa rasanya udah eneg ngeliat ...       in   \n",
       "2       Today^s meeting between #HongKongProtesters an...       en   \n",
       "3       The harder the conflict, the more glorious the...       en   \n",
       "4       RT @OscarLe27560696: China own news paper (HK)...       en   \n",
       "...                                                   ...      ...   \n",
       "208520  \\u9032\\u5316\\u7248\\u7684\\u300c\\u8fd4\\u6821\\u30...       ja   \n",
       "208521  \\u9999\\u6e2f\\u3001\\u30c7\\u30e2\\u3084\\u96c6\\u4f...       ja   \n",
       "208522  @3Yannn @Dreamy31875054 \\u9999\\u6e2f\\u306e\\u65...       ja   \n",
       "208523  \\u5149\\u5fa9\\u9999\\u6e2f https://t.co/c8hS9MwuWf        ja   \n",
       "208524  \\u300a\\u5149\\u5fa9\\u9999\\u6e2f\\u300bVR\\u30b1\\u...       ja   \n",
       "\n",
       "                 created_at              user_id  \\\n",
       "0       2019-09-19 00:20:49           2856307798   \n",
       "1       2019-09-19 00:20:17            134116652   \n",
       "2       2019-09-19 00:20:07           2573661730   \n",
       "3       2019-09-19 00:20:01           4782149654   \n",
       "4       2019-09-19 00:19:06  1126143126605746176   \n",
       "...                     ...                  ...   \n",
       "208520  2019-10-26 16:32:16   804543102132158464   \n",
       "208521  2019-10-26 15:52:07            157583088   \n",
       "208522  2019-10-26 15:30:09  1168070637329346560   \n",
       "208523  2019-10-26 06:33:42   908206346490626048   \n",
       "208524  2019-10-26 04:11:33           4401027434   \n",
       "\n",
       "                                             name      screen_name  \\\n",
       "0       \\U0001f308#ERAnow!!!gerrymand cost 16\\U0          WebOften   \n",
       "1                                   hari pramuka     hadipramudika   \n",
       "2                                    Lucas Notch     lucas_and_mew   \n",
       "3                                    Libertalia!   libertalia_band   \n",
       "4        sanshuicc\\U0001f1ed\\U0001f1f0\\U0001f64f        sanshuicc1   \n",
       "...                                           ...              ...   \n",
       "208520                  \\u527f\\u532a\\u5b78\\u9662         laichinan   \n",
       "208521                                   Shota T         sasakurex   \n",
       "208522  FreedOmHK @ \\U0001f1ef\\U0001f1f5\\U0001f1       jhm_freedom   \n",
       "208523                                  Lazijaja        lazijajaja   \n",
       "208524  Carl Chan @\\u9999\\u6e2f\\u4fdd\\u885b\\u623        CarlChan19   \n",
       "\n",
       "                                              description  \\\n",
       "0       Just another #feminist\\u2640\\ufe0f #green \\U00...   \n",
       "1                         kalo ngepil harus digerus dulu    \n",
       "2                                          Gamer. Otaku.    \n",
       "3       Libertalia writes, records and performs modern...   \n",
       "4                   \\u4f20\\u64ad\\u7206\\u6599\\u9769\\u547d    \n",
       "...                                                   ...   \n",
       "208520  \\u527f\\u532a\\u5c1a\\u672a\\u6210\\u529f\\uff0c\\u54...   \n",
       "208521  \\u6620\\u753b\\u30fb\\u97f3\\u697d\\u30fb\\u30a2\\u30...   \n",
       "208522  I^m JP and Hongkong Mixd , I^m supportive HK!!...   \n",
       "208523                                            Sleepy    \n",
       "208524  \\u6240\\u5728\\u5730\\uff1a\\u9999\\u6e2f\\u3002\\n\\u...   \n",
       "\n",
       "                               location  n_followers  n_friends  \\\n",
       "0                                             2462.0     5002.0   \n",
       "1                                              146.0      105.0   \n",
       "2                                               66.0      218.0   \n",
       "3       La R\\xe9publique de Libertalia        7990.0       10.0   \n",
       "4                                               13.0       66.0   \n",
       "...                                 ...          ...        ...   \n",
       "208520           Taichung City, Taiwan        5770.0      647.0   \n",
       "208521                          Japan          122.0       68.0   \n",
       "208522                                         265.0      284.0   \n",
       "208523                       Hong Kong           1.0       41.0   \n",
       "208524                                         687.0      158.0   \n",
       "\n",
       "        n_tweets_user  polarity  \\\n",
       "0             82945.0  0.444381   \n",
       "1              5666.0  0.709279   \n",
       "2              7558.0  0.196698   \n",
       "3             14227.0  0.968098   \n",
       "4               123.0  0.687024   \n",
       "...               ...       ...   \n",
       "208520         5808.0  0.746148   \n",
       "208521         4485.0  0.746148   \n",
       "208522          381.0  0.746148   \n",
       "208523           25.0  0.100058   \n",
       "208524          667.0  0.746148   \n",
       "\n",
       "                                               tweet_text  \n",
       "0       #inners Chris^ talk re: Pelosi^s skills\\u2014s...  \n",
       "1       @atri_septiani rasa rasanya udah eneg ngeliat ...  \n",
       "2       Today^s meeting between #HongKongProtesters an...  \n",
       "3       The harder the conflict, the more glorious the...  \n",
       "4       RT @OscarLe27560696: China own news paper (HK)...  \n",
       "...                                                   ...  \n",
       "208520  \\u9032\\u5316\\u7248\\u7684\\u300c\\u8fd4\\u6821\\u30...  \n",
       "208521  \\u9999\\u6e2f\\u3001\\u30c7\\u30e2\\u3084\\u96c6\\u4f...  \n",
       "208522  @3Yannn @Dreamy31875054 \\u9999\\u6e2f\\u306e\\u65...  \n",
       "208523  \\u5149\\u5fa9\\u9999\\u6e2f https://t.co/c8hS9MwuWf   \n",
       "208524  \\u300a\\u5149\\u5fa9\\u9999\\u6e2f\\u300bVR\\u30b1\\u...  \n",
       "\n",
       "[417051 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"@atri_septiani rasa rasanya udah eneg ngelia\"\n",
    "test = test.encode('utf-8').decode('utf-8','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>n_friends</th>\n",
       "      <th>n_tweets_user</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1174478413253545984</td>\n",
       "      <td>1174478413253545984</td>\n",
       "      <td>#inners Chris^ talk re: Pelosi^s skills\\u2014s...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:20:49</td>\n",
       "      <td>2856307798</td>\n",
       "      <td>\\U0001f308#ERAnow!!!gerrymand cost 16\\U0</td>\n",
       "      <td>WebOften</td>\n",
       "      <td>Just another #feminist\\u2640\\ufe0f #green \\U00...</td>\n",
       "      <td></td>\n",
       "      <td>2462.0</td>\n",
       "      <td>5002.0</td>\n",
       "      <td>82945.0</td>\n",
       "      <td>0.444381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1174478279719260160</td>\n",
       "      <td>1174478279719260160</td>\n",
       "      <td>@atri_septiani rasa rasanya udah eneg ngeliat ...</td>\n",
       "      <td>in</td>\n",
       "      <td>2019-09-19 00:20:17</td>\n",
       "      <td>134116652</td>\n",
       "      <td>hari pramuka</td>\n",
       "      <td>hadipramudika</td>\n",
       "      <td>kalo ngepil harus digerus dulu</td>\n",
       "      <td></td>\n",
       "      <td>146.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5666.0</td>\n",
       "      <td>0.709279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1174478237105348608</td>\n",
       "      <td>1174478237105348608</td>\n",
       "      <td>Today^s meeting between #HongKongProtesters an...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:20:07</td>\n",
       "      <td>2573661730</td>\n",
       "      <td>Lucas Notch</td>\n",
       "      <td>lucas_and_mew</td>\n",
       "      <td>Gamer. Otaku.</td>\n",
       "      <td></td>\n",
       "      <td>66.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>7558.0</td>\n",
       "      <td>0.196698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1174478213466206208</td>\n",
       "      <td>1174478213466206208</td>\n",
       "      <td>The harder the conflict, the more glorious the...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:20:01</td>\n",
       "      <td>4782149654</td>\n",
       "      <td>Libertalia!</td>\n",
       "      <td>libertalia_band</td>\n",
       "      <td>Libertalia writes, records and performs modern...</td>\n",
       "      <td>La R\\xe9publique de Libertalia</td>\n",
       "      <td>7990.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14227.0</td>\n",
       "      <td>0.968098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1174477984620630016</td>\n",
       "      <td>1174477984620630016</td>\n",
       "      <td>RT @OscarLe27560696: China own news paper (HK)...</td>\n",
       "      <td>en</td>\n",
       "      <td>2019-09-19 00:19:06</td>\n",
       "      <td>1126143126605746176</td>\n",
       "      <td>sanshuicc\\U0001f1ed\\U0001f1f0\\U0001f64f</td>\n",
       "      <td>sanshuicc1</td>\n",
       "      <td>\\u4f20\\u64ad\\u7206\\u6599\\u9769\\u547d</td>\n",
       "      <td></td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.687024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208520</td>\n",
       "      <td>1188131237719724034</td>\n",
       "      <td>1188131237719724034</td>\n",
       "      <td>\\u9032\\u5316\\u7248\\u7684\\u300c\\u8fd4\\u6821\\u30...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 16:32:16</td>\n",
       "      <td>804543102132158464</td>\n",
       "      <td>\\u527f\\u532a\\u5b78\\u9662</td>\n",
       "      <td>laichinan</td>\n",
       "      <td>\\u527f\\u532a\\u5c1a\\u672a\\u6210\\u529f\\uff0c\\u54...</td>\n",
       "      <td>Taichung City, Taiwan</td>\n",
       "      <td>5770.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>5808.0</td>\n",
       "      <td>0.746148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208521</td>\n",
       "      <td>1188121135239483395</td>\n",
       "      <td>1188121135239483395</td>\n",
       "      <td>\\u9999\\u6e2f\\u3001\\u30c7\\u30e2\\u3084\\u96c6\\u4f...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 15:52:07</td>\n",
       "      <td>157583088</td>\n",
       "      <td>Shota T</td>\n",
       "      <td>sasakurex</td>\n",
       "      <td>\\u6620\\u753b\\u30fb\\u97f3\\u697d\\u30fb\\u30a2\\u30...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>122.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4485.0</td>\n",
       "      <td>0.746148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208522</td>\n",
       "      <td>1188115606098206720</td>\n",
       "      <td>1188115606098206720</td>\n",
       "      <td>@3Yannn @Dreamy31875054 \\u9999\\u6e2f\\u306e\\u65...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 15:30:09</td>\n",
       "      <td>1168070637329346560</td>\n",
       "      <td>FreedOmHK @ \\U0001f1ef\\U0001f1f5\\U0001f1</td>\n",
       "      <td>jhm_freedom</td>\n",
       "      <td>I^m JP and Hongkong Mixd , I^m supportive HK!!...</td>\n",
       "      <td></td>\n",
       "      <td>265.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.746148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208523</td>\n",
       "      <td>1187980605394276352</td>\n",
       "      <td>1187980605394276352</td>\n",
       "      <td>\\u5149\\u5fa9\\u9999\\u6e2f https://t.co/c8hS9MwuWf</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 06:33:42</td>\n",
       "      <td>908206346490626048</td>\n",
       "      <td>Lazijaja</td>\n",
       "      <td>lazijajaja</td>\n",
       "      <td>Sleepy</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.100058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208524</td>\n",
       "      <td>1187944830049013760</td>\n",
       "      <td>1187944830049013760</td>\n",
       "      <td>\\u300a\\u5149\\u5fa9\\u9999\\u6e2f\\u300bVR\\u30b1\\u...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2019-10-26 04:11:33</td>\n",
       "      <td>4401027434</td>\n",
       "      <td>Carl Chan @\\u9999\\u6e2f\\u4fdd\\u885b\\u623</td>\n",
       "      <td>CarlChan19</td>\n",
       "      <td>\\u6240\\u5728\\u5730\\uff1a\\u9999\\u6e2f\\u3002\\n\\u...</td>\n",
       "      <td></td>\n",
       "      <td>687.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>0.746148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417051 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Unnamed: 0                   id  \\\n",
       "0       1174478413253545984  1174478413253545984   \n",
       "1       1174478279719260160  1174478279719260160   \n",
       "2       1174478237105348608  1174478237105348608   \n",
       "3       1174478213466206208  1174478213466206208   \n",
       "4       1174477984620630016  1174477984620630016   \n",
       "...                     ...                  ...   \n",
       "208520  1188131237719724034  1188131237719724034   \n",
       "208521  1188121135239483395  1188121135239483395   \n",
       "208522  1188115606098206720  1188115606098206720   \n",
       "208523  1187980605394276352  1187980605394276352   \n",
       "208524  1187944830049013760  1187944830049013760   \n",
       "\n",
       "                                                    tweet language  \\\n",
       "0       #inners Chris^ talk re: Pelosi^s skills\\u2014s...       en   \n",
       "1       @atri_septiani rasa rasanya udah eneg ngeliat ...       in   \n",
       "2       Today^s meeting between #HongKongProtesters an...       en   \n",
       "3       The harder the conflict, the more glorious the...       en   \n",
       "4       RT @OscarLe27560696: China own news paper (HK)...       en   \n",
       "...                                                   ...      ...   \n",
       "208520  \\u9032\\u5316\\u7248\\u7684\\u300c\\u8fd4\\u6821\\u30...       ja   \n",
       "208521  \\u9999\\u6e2f\\u3001\\u30c7\\u30e2\\u3084\\u96c6\\u4f...       ja   \n",
       "208522  @3Yannn @Dreamy31875054 \\u9999\\u6e2f\\u306e\\u65...       ja   \n",
       "208523  \\u5149\\u5fa9\\u9999\\u6e2f https://t.co/c8hS9MwuWf        ja   \n",
       "208524  \\u300a\\u5149\\u5fa9\\u9999\\u6e2f\\u300bVR\\u30b1\\u...       ja   \n",
       "\n",
       "                 created_at              user_id  \\\n",
       "0       2019-09-19 00:20:49           2856307798   \n",
       "1       2019-09-19 00:20:17            134116652   \n",
       "2       2019-09-19 00:20:07           2573661730   \n",
       "3       2019-09-19 00:20:01           4782149654   \n",
       "4       2019-09-19 00:19:06  1126143126605746176   \n",
       "...                     ...                  ...   \n",
       "208520  2019-10-26 16:32:16   804543102132158464   \n",
       "208521  2019-10-26 15:52:07            157583088   \n",
       "208522  2019-10-26 15:30:09  1168070637329346560   \n",
       "208523  2019-10-26 06:33:42   908206346490626048   \n",
       "208524  2019-10-26 04:11:33           4401027434   \n",
       "\n",
       "                                             name      screen_name  \\\n",
       "0       \\U0001f308#ERAnow!!!gerrymand cost 16\\U0          WebOften   \n",
       "1                                   hari pramuka     hadipramudika   \n",
       "2                                    Lucas Notch     lucas_and_mew   \n",
       "3                                    Libertalia!   libertalia_band   \n",
       "4        sanshuicc\\U0001f1ed\\U0001f1f0\\U0001f64f        sanshuicc1   \n",
       "...                                           ...              ...   \n",
       "208520                  \\u527f\\u532a\\u5b78\\u9662         laichinan   \n",
       "208521                                   Shota T         sasakurex   \n",
       "208522  FreedOmHK @ \\U0001f1ef\\U0001f1f5\\U0001f1       jhm_freedom   \n",
       "208523                                  Lazijaja        lazijajaja   \n",
       "208524  Carl Chan @\\u9999\\u6e2f\\u4fdd\\u885b\\u623        CarlChan19   \n",
       "\n",
       "                                              description  \\\n",
       "0       Just another #feminist\\u2640\\ufe0f #green \\U00...   \n",
       "1                         kalo ngepil harus digerus dulu    \n",
       "2                                          Gamer. Otaku.    \n",
       "3       Libertalia writes, records and performs modern...   \n",
       "4                   \\u4f20\\u64ad\\u7206\\u6599\\u9769\\u547d    \n",
       "...                                                   ...   \n",
       "208520  \\u527f\\u532a\\u5c1a\\u672a\\u6210\\u529f\\uff0c\\u54...   \n",
       "208521  \\u6620\\u753b\\u30fb\\u97f3\\u697d\\u30fb\\u30a2\\u30...   \n",
       "208522  I^m JP and Hongkong Mixd , I^m supportive HK!!...   \n",
       "208523                                            Sleepy    \n",
       "208524  \\u6240\\u5728\\u5730\\uff1a\\u9999\\u6e2f\\u3002\\n\\u...   \n",
       "\n",
       "                               location  n_followers  n_friends  \\\n",
       "0                                             2462.0     5002.0   \n",
       "1                                              146.0      105.0   \n",
       "2                                               66.0      218.0   \n",
       "3       La R\\xe9publique de Libertalia        7990.0       10.0   \n",
       "4                                               13.0       66.0   \n",
       "...                                 ...          ...        ...   \n",
       "208520           Taichung City, Taiwan        5770.0      647.0   \n",
       "208521                          Japan          122.0       68.0   \n",
       "208522                                         265.0      284.0   \n",
       "208523                       Hong Kong           1.0       41.0   \n",
       "208524                                         687.0      158.0   \n",
       "\n",
       "        n_tweets_user  polarity  \n",
       "0             82945.0  0.444381  \n",
       "1              5666.0  0.709279  \n",
       "2              7558.0  0.196698  \n",
       "3             14227.0  0.968098  \n",
       "4               123.0  0.687024  \n",
       "...               ...       ...  \n",
       "208520         5808.0  0.746148  \n",
       "208521         4485.0  0.746148  \n",
       "208522          381.0  0.746148  \n",
       "208523           25.0  0.100058  \n",
       "208524          667.0  0.746148  \n",
       "\n",
       "[417051 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:/Users/ericluo04/Documents/GitHub/Bots-Project/Code/2. HK Training/polarities/master_new_1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add punctuation to stopwords list\n",
    "stoplist = stopwords.words('english') + list(string.punctuation)\n",
    "stoplist = set(stoplist)\n",
    "\n",
    "# lemmatizer function\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# stemmer function\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to expand contractions\n",
    "# https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Stemming cuts off the end or the beginning of the word, taking into account a list of common prefixes and suffixes. Costs are overstemming (eg. universal, universities, and universe all stem to \"univers\") and understemming (e.g. data and datum stem to \"dat\" and \"datu,\" respectively). \n",
    "\n",
    "> Lemmatization takes into consideration the morphological analysis of the words (e.g. resolves is and are to “be”). This is usually the preferred way of cleaning text data. Use stemming if there are speed or memory constraints. \n",
    "\n",
    "> Spelling correction is based on Peter Norvig’s “How to Write a Spelling Corrector” (http://norvig.com/spell-correct.html). It is about 70% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing function (with different versions: lemmatize, stem, spell check with lemmatization)\n",
    "# order of steps:\n",
    "# 1. lower case\n",
    "# 2. expand contractions\n",
    "# (2.5 spell check)\n",
    "# 3. tokenize\n",
    "# 4. remove words in stop words list (with punctuation) and digits - ignores case\n",
    "#    can specify the most n and least n frequency words to filter out (before stop word removal)\n",
    "# 5. lemmatize/stem\n",
    "\n",
    "def preprocess(text, version=\"lemmaSpell\", rareFilter=0, freqFilter=0):\n",
    "#     freq, rare = [], []\n",
    "#     if freqFilter > 0:\n",
    "#         freq = list((pd.Series(' '.join(df['content'].lower()).split()).value_counts()[:freqFilter]).index)\n",
    "#     if rareFilter > 0:\n",
    "#         rare = list((pd.Series(' '.join(df['content'].lower()).split()).value_counts()[-rareFilter:]).index)\n",
    "#     stoplist2 = set(list(stoplist) + freq + rare)\n",
    "    \n",
    "    if version == \"lemmatize\":\n",
    "        return [lemmatizer.lemmatize(word)\n",
    "            for word in word_tokenize(decontracted(text.lower()))\n",
    "            if word not in stoplist and not word.isdigit()]\n",
    "    \n",
    "    if version == \"stem\":\n",
    "        return [stemmer.stem(word) \n",
    "            for word in word_tokenize(decontracted(text.lower())) \n",
    "            if word not in stoplist and not word.isdigit()]\n",
    "    \n",
    "    if version == \"lemmaSpell\":\n",
    "        return [lemmatizer.lemmatize(word)\n",
    "            for word in word_tokenize(''.join(TextBlob(decontracted(text.lower())).correct()))\n",
    "            if word not in stoplist and not word.isdigit()]\n",
    "    \n",
    "def saProcess(text):\n",
    "    return ''.join(TextBlob(decontracted(text)).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply preprocessing functions to dataframe column 'content' with text values\n",
    "clean = df['content'].apply(preprocess, version=\"lemmaSpell\", rareFilter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df for sentiment analysis (expand contractions and spell correct)\n",
    "# use this dataframe if you want!\n",
    "dfSA = df['content'].apply(saProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Sentiment Analysis (English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __NLTK’s Vader__ sentiment analysis tool uses a bag of words approach (a lookup table of positive and negative words). Vader accounts for punctuation (e.g. !), capitalization, intensifiers (e.g. extremely, very, absolutely), conjunctions (e.g. but), tri-grams for negation (so that negation is only taken into account within three words of the negating word), emojis, slang, and emoticons. This means that you should NOT clean your text - usually extraneous info is taken into account for sentiment analysis. Vader has shown to be very successful when dealing with social media, NY Times editorials, movie reviews, and product reviews.\n",
    "\n",
    "2. __TextBlob__ works like Vader (bag of words classifier), but TextBlob also has Subjectivity Analysis (scores how factual/opinionated text is). However, it doesn’t contain the heuristics that NLTK has, and so it won’t intensify or negate a sentence’s sentiment.\n",
    "\n",
    "3. __Flair__ is based on a character-level LSTM neural network which takes sequences of letters and words into account when predicting. So, it can predict sentiment for words that the algorithm has never seen before (e.g. typos). Flair also takes into account negation and intensifiers. \n",
    "\n",
    "Vader and TextBlob are computationally cheap and will run very quickly over large amounts of data. Flair, on the other hand, is extremely slow and takes a rather long time to download the pre-trained model, but probably has state-of-the-art performance. For all three tools, sentiment scores range between negative one and positive one: -1 = Negative, 0 = Neutral, 1 = Positive. Blob Text's subjectivity score ranges between zero and one: 0 = very objective and 1 = very subjective.\n",
    "\n",
    "I recommend Vader for sentiment analysis and TextBlob for subjectivity analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in vader sentiment analysis function\n",
    "vaderSA = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vaderSentiment(data, column):\n",
    "    # initialize lists\n",
    "    compound = []\n",
    "    neg = []\n",
    "    neu = []\n",
    "    pos = []\n",
    "    # fill in lists with sentiment\n",
    "    for index, row in data.iterrows():\n",
    "        compound.append(vaderSA.polarity_scores(row[column])['compound'])    # total sentiment (neg+neu+pos)\n",
    "        neg.append(vaderSA.polarity_scores(row[column])['neg'])              # negative sentiment\n",
    "        neu.append(vaderSA.polarity_scores(row[column])['neu'])              # neutral sentiment\n",
    "        pos.append(vaderSA.polarity_scores(row[column])['pos'])              # positive sentiment\n",
    "    data['sentiment_vader'] = compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaderSentiment(df, 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blobSentiment(data, column):\n",
    "    # initialize lists\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "    # fill in lists with sentiment\n",
    "    for index, row in data.iterrows():\n",
    "        polarity.append(TextBlob(row[column]).sentiment.polarity)             # sentiment\n",
    "        subjectivity.append(TextBlob(row[column]).sentiment.subjectivity)     # subjectivity\n",
    "    data['sentiment_blob'] = polarity\n",
    "    data['subjectivity_blob'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobSentiment(df, 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-22 15:48:24,893 loading file C:\\Users\\ericluo04\\.flair\\models\\imdb-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "# read in flair sentiment analysis function\n",
    "flairClassifier = flair.models.TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flairSentiment(data, column):\n",
    "    values = []\n",
    "    for index, row in data.iterrows():\n",
    "        tokenized = Sentence(row[column])\n",
    "        flairClassifier.predict(tokenized)\n",
    "        values.append(tokenized.labels[0].score)\n",
    "    data['sentiment_flair'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairSentiment(df, 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covar</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>sentiment_blob</th>\n",
       "      <th>subjectivity_blob</th>\n",
       "      <th>sentiment_flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I really like that</td>\n",
       "      <td>0.4201</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.998603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>that'S pretty COOL!!!</td>\n",
       "      <td>0.7969</td>\n",
       "      <td>0.466797</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.517429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I do not think that's very nice!</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>What about that?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>This data is super interesting</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.913916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>This datum is super interesting.</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.997246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>University should be for everyone</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>The universe should be for everyone</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>I HATE HATE HATE doing homework.</td>\n",
       "      <td>-0.9360</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.985813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't seem to fall asleep</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "      <td>-0.7959</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.992943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>Chcking for spll corrction</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Mspelling wrds on prpose.</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>PunCTUaTION TEsT!!?!</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>Gbberish here @PerezHilton @annarosekerr @Isaa...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.890191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>I am not HAPPY at all</td>\n",
       "      <td>-0.5485</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    covar                                            content  sentiment_vader  \\\n",
       "0       1                                 I really like that           0.4201   \n",
       "1       2                              that'S pretty COOL!!!           0.7969   \n",
       "2       3                   I do not think that's very nice!           0.5244   \n",
       "3       1                                   What about that?           0.0000   \n",
       "4       2                     This data is super interesting           0.7650   \n",
       "5       2                  This datum is super interesting.            0.7650   \n",
       "6       3                 University should be for everyone\n",
       "           0.0000   \n",
       "7       3               The universe should be for everyone\n",
       "           0.0000   \n",
       "8       2                  I HATE HATE HATE doing homework.           -0.9360   \n",
       "9       1                        I can't seem to fall asleep           0.0000   \n",
       "10      1  Ugh! I have to beat this stupid song to get to...          -0.7959   \n",
       "11      3                         Chcking for spll corrction           0.0000   \n",
       "12      2                         Mspelling wrds on prpose.            0.0000   \n",
       "13      1                               PunCTUaTION TEsT!!?!           0.0000   \n",
       "14      1  Gbberish here @PerezHilton @annarosekerr @Isaa...           0.0000   \n",
       "15      3                              I am not HAPPY at all          -0.5485   \n",
       "\n",
       "    sentiment_blob  subjectivity_blob  sentiment_flair  \n",
       "0         0.200000           0.200000         0.998603  \n",
       "1         0.466797           0.825000         0.517429  \n",
       "2         0.975000           1.000000         0.946681  \n",
       "3         0.000000           0.000000         0.997439  \n",
       "4         0.416667           0.583333         0.913916  \n",
       "5         0.416667           0.583333         0.997246  \n",
       "6         0.000000           0.000000         0.992943  \n",
       "7         0.000000           0.000000         0.993712  \n",
       "8        -0.800000           0.900000         0.985813  \n",
       "9         0.000000           0.000000         0.697716  \n",
       "10       -0.400000           0.500000         0.992943  \n",
       "11        0.000000           0.000000         0.857787  \n",
       "12        0.000000           0.000000         0.999721  \n",
       "13        0.000000           0.000000         0.836005  \n",
       "14        0.000000           0.000000         0.890191  \n",
       "15       -0.400000           1.000000         0.893359  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path + 'sentimentData.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Sentiment Analysis (Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can simply translate the Chinese into English using google translate. Now, with the translated column, we can use any of the methods from the English sentiment analysis section.\n",
    "2. SnowNLP is a Simplified Chinese Text processing module. This versatile package that can do sentiment analysis, POS tagging, convert to pinyin/simplified, and choose keywords/key sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data frame (chinese) - excel file instead of csv to preserve chinese characters\n",
    "dfChinese = pd.read_excel(path + \"testSentimentChinese.xlsx\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google translate function\n",
    "translator = Translator()\n",
    "\n",
    "def translate(data, column):\n",
    "    # initialize lists\n",
    "    translated = []\n",
    "    # fill in lists with sentiment\n",
    "    for index, row in data.iterrows():\n",
    "        translated.append(translator.translate(row[column]).text)\n",
    "    data['translation'] = translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(dfChinese, 'content_Chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaderSentiment(dfChinese, 'translation')\n",
    "blobSentiment(dfChinese, 'translation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snowNLP (0=Negative, 1=Positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowSentiment(data, column):\n",
    "    # initialize lists\n",
    "    snow = []\n",
    "    # fill in lists with sentiment\n",
    "    for index, row in data.iterrows():\n",
    "        snow.append(SnowNLP(SnowNLP(row[column]).han).sentiments)             # sentiment\n",
    "    data['sentiment_snow'] = snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowSentiment(dfChinese, 'content_Chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>covar</th>\n",
       "      <th>content</th>\n",
       "      <th>content_Chinese</th>\n",
       "      <th>translation</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>sentiment_blob</th>\n",
       "      <th>subjectivity_blob</th>\n",
       "      <th>sentiment_snow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I really like that</td>\n",
       "      <td>我真的很喜欢</td>\n",
       "      <td>I really like</td>\n",
       "      <td>0.4201</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.859321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>That's pretty COOL!!!</td>\n",
       "      <td>太酷了！！！</td>\n",
       "      <td>so cool! ! !</td>\n",
       "      <td>0.5376</td>\n",
       "      <td>0.683594</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.236241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>I do not think that's very nice!</td>\n",
       "      <td>我认为那不是很好！</td>\n",
       "      <td>I think it is not very good!</td>\n",
       "      <td>-0.4432</td>\n",
       "      <td>-0.336538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.650908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>What about that?</td>\n",
       "      <td>那个怎么样？</td>\n",
       "      <td>How about that one?</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate doing homework.</td>\n",
       "      <td>我讨厌做作业。</td>\n",
       "      <td>I hate homework.</td>\n",
       "      <td>-0.5719</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.894835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>I am not HAPPY at all</td>\n",
       "      <td>我一点都不开心</td>\n",
       "      <td>I'm not happy</td>\n",
       "      <td>-0.4585</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>I have no opinion.</td>\n",
       "      <td>我没意见</td>\n",
       "      <td>I have no opinion</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>That is a good idea.</td>\n",
       "      <td>那是一个好主意。</td>\n",
       "      <td>It is a good idea.</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.758971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>I have to buy a textbook.</td>\n",
       "      <td>我得买一本教科书。</td>\n",
       "      <td>I have to buy textbooks.</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>The weather is very warm.</td>\n",
       "      <td>天气很温暖。</td>\n",
       "      <td>The weather was very warm.</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.926272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   covar                           content content_Chinese  \\\n",
       "0      1                I really like that          我真的很喜欢   \n",
       "1      2             That's pretty COOL!!!          太酷了！！！   \n",
       "2      3  I do not think that's very nice!       我认为那不是很好！   \n",
       "3      1                  What about that?          那个怎么样？   \n",
       "4      2           I hate doing homework.          我讨厌做作业。   \n",
       "5      2             I am not HAPPY at all         我一点都不开心   \n",
       "6      3               I have no opinion.             我没意见   \n",
       "7      3             That is a good idea.         那是一个好主意。   \n",
       "8      2        I have to buy a textbook.        我得买一本教科书。   \n",
       "9      1        The weather is very warm.           天气很温暖。   \n",
       "\n",
       "                    translation  sentiment_vader  sentiment_blob  \\\n",
       "0                 I really like           0.4201        0.200000   \n",
       "1                  so cool! ! !           0.5376        0.683594   \n",
       "2  I think it is not very good!          -0.4432       -0.336538   \n",
       "3           How about that one?           0.0000        0.000000   \n",
       "4              I hate homework.          -0.5719       -0.800000   \n",
       "5                 I'm not happy          -0.4585       -0.400000   \n",
       "6             I have no opinion          -0.2960        0.000000   \n",
       "7            It is a good idea.           0.4404        0.700000   \n",
       "8      I have to buy textbooks.           0.0000        0.000000   \n",
       "9    The weather was very warm.           0.2944        0.780000   \n",
       "\n",
       "   subjectivity_blob  sentiment_snow  \n",
       "0           0.200000        0.859321  \n",
       "1           0.650000        0.236241  \n",
       "2           0.461538        0.650908  \n",
       "3           0.000000        0.526233  \n",
       "4           0.900000        0.894835  \n",
       "5           1.000000        0.569013  \n",
       "6           0.000000        0.238207  \n",
       "7           0.600000        0.758971  \n",
       "8           0.000000        0.613107  \n",
       "9           0.780000        0.926272  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfChinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfChinese.to_csv(path + 'sentimentDataChinese.csv', index=None, header=True, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My personal exploration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['老虎张开大嘴就要把狐狸吃掉',\n",
       " '我可是玉皇大帝派来管理百兽的兽王',\n",
       " '玉皇大帝是决不会放过你的\"',\n",
       " '一只饥饿的老虎逮住了一只狐狸',\n",
       " '你要是吃了我']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = SnowNLP(u'在茂密的大森林里，一只饥饿的老虎逮住了一只狐狸。老虎张开大嘴就要把狐狸吃掉。\"慢着\"！狐狸虽然很害怕但还是装出一副很神气的样子说，\"你知道我是谁吗？我可是玉皇大帝派来管理百兽的兽王，你要是吃了我，玉皇大帝是决不会放过你的\"。')\n",
    "\n",
    "# POS tagging\n",
    "list(s.tags)\n",
    "# convert to pinyin\n",
    "s.pinyin\n",
    "# convert to simplified\n",
    "s.han\n",
    "# split apart into individual sentences\n",
    "s.sentences\n",
    "# choose 5 keywords\n",
    "s.keywords(5)\n",
    "# choose 5 key sentences\n",
    "s.summary(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('test', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('if', 'IN'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('tagging', 'VBG'),\n",
       " ('works', 'NNS')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English POS tagging\n",
    "s_eng = TextBlob(\"Hello, this is a test to see if part of speech tagging works.\")\n",
    "s_eng.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Hello/NNP\n",
      "  this/DT\n",
      "  is/VBZ\n",
      "  (NP a/DT test/NN)\n",
      "  to/TO\n",
      "  see/VB\n",
      "  if/IN\n",
      "  (NP part/NN)\n",
      "  of/IN\n",
      "  (NP speech/NN)\n",
      "  tagging/VBG\n",
      "  works/NNS)\n"
     ]
    }
   ],
   "source": [
    "# chunking\n",
    "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result = rp.parse(s_eng.tags)\n",
    "print(result)\n",
    "#result.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
